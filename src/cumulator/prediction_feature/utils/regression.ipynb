{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import redirect_stdout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def decision_tree(X_train,X_test,y_train,y_test):\n",
    "    clf = DecisionTreeRegressor()\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    return clf, np.sqrt(mean_squared_error(y_test, clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def random_forest(X_train,X_test,y_train,y_test):\n",
    "    #hyperparameter-tuning\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [100, 200, 300, 1000]\n",
    "\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(50, 100,10)]\n",
    "\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10]\n",
    "\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    # Create the random grid\n",
    "    grid = {'n_estimators': n_estimators,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'bootstrap': bootstrap\n",
    "            }\n",
    "\n",
    "    clf = RandomForestRegressor()\n",
    "    clf_cv = RandomizedSearchCV(estimator = clf, param_distributions = grid, cv = 5, random_state=42)\n",
    "    # Fit the random search model\n",
    "    clf_cv=clf_cv.fit(X_train, y_train)\n",
    "    return clf_cv, np.sqrt(mean_squared_error(y_test, clf_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def linear_regression(X_train, X_test, y_train, y_test):\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf, np.sqrt(mean_squared_error(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.4 Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "import numpy as np\n",
    "\n",
    "def neural_network(X_train,X_test,y_train,y_test):\n",
    "    # Create the random grid\n",
    "    grid = [\n",
    "    {'solver': ['sgd','adam','lbfgs'],\n",
    "     'learning_rate_init': [0.000001],\n",
    "     'max_iter': [10000],\n",
    "     'hidden_layer_sizes': [(50,40,30),(30, 30),(10,10,10)],\n",
    "     'activation': ['logistic', 'relu'],\n",
    "     'alpha': [0.0001, 0.001, 0.005],\n",
    "     }\n",
    "]\n",
    "    clf = MLPRegressor()\n",
    "    clf_cv = RandomizedSearchCV(estimator = clf, param_distributions = grid, cv = 5, random_state=42)\n",
    "    clf_cv=clf_cv.fit(X_train,y_train)\n",
    "    return clf_cv,np.sqrt(mean_squared_error(y_test, clf_cv.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_autoML(X_train,X_test,y_train,y_test):\n",
    "    automl = AutoML(eval_metric='rmse', explain_level=1, top_models_to_improve=4, random_state=2, optuna_verbose=False)\n",
    "    automl.fit(X_train, y_train)\n",
    "    # compute the MSE on test data\n",
    "    predictions = automl.predict(X_test)\n",
    "    return np.sqrt(mean_squared_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ml_dataset.csv')\n",
    "dataset_cleaned  = dataset[dataset.algo != 'Ensemble']\n",
    "dataset_cleaned = dataset_cleaned.drop(columns=['did'])\n",
    "dataset_cleaned['consumption']=dataset_cleaned['time']*dataset_cleaned['TDP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos=dataset_cleaned['algo'].unique()\n",
    "list_models=[{\"name\":\"Decision Tree\", \"clf\":decision_tree}, {\"name\":\"Random Forest\", \"clf\":random_forest},{\"name\":\"Linear Regression\",  \"clf\":linear_regression}, {\"name\":\"Nerual Network\", \"clf\":neural_network}]\n",
    "names=[x['name'] for x in list_models]\n",
    "rmse_per_algo=[]\n",
    "for algo in algos:\n",
    "    clf=[]\n",
    "    print(f'-------------DATASET {algo}--------------')\n",
    "    df=dataset_cleaned[dataset_cleaned['algo']==algo]\n",
    "    df=dataset_cleaned.drop(columns=['algo','country','TDP','consumption','time'])\n",
    "    df=df.dropna()\n",
    "    #log scaling for heavy-tailed distributions\n",
    "    for i in df.columns:\n",
    "        if i!='F1':\n",
    "            df[i]=np.log(1+df[i])\n",
    "    X=df.loc[:,df.columns!='F1'].to_numpy()\n",
    "    y=df['F1'].to_numpy()\n",
    "    #Train-Test splitting\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    #selecting 7 features with highest mutual_information\n",
    "    selector = SelectKBest(mutual_info_regression, k=7).fit(X_train,y_train)\n",
    "    support = selector.get_support()\n",
    "    X_train = X_train[:,support]\n",
    "    X_test = X_test[:,support]\n",
    "    #run_autoML(X_train,X_test,y_train,y_test)\n",
    "    rmse_list=[]\n",
    "    clf_list=[]\n",
    "    #models training\n",
    "    for model in list_models:\n",
    "        clf,rmse=model['clf'](X_train,X_test, y_train,y_test)\n",
    "        rmse_list.append(rmse)\n",
    "        clf_list.append(clf)\n",
    "    data={'Model':names, 'RMSE':rmse_list}\n",
    "    df_plot=pd.DataFrame(data)\n",
    "    #plotting\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=\"Model\", y=\"RMSE\", data=df_plot)\n",
    "    filename='_'.join(algo.split())\n",
    "    plt.savefig(f'./plots/F1_model_{filename}.png')\n",
    "    rmse_per_algo.append(np.min(np.array(rmse_list)))\n",
    "    #saving best model\n",
    "    pickle.dump(clf_list[np.argmin(np.array(rmse_list))], open(f'./models/F1_model_{filename}.sav', 'wb'))\n",
    "    #saving support features\n",
    "    pickle.dump(support, open(f'./models/support_F1_{filename}.pkl', 'wb'))\n",
    "    print(f'Min rmse:{np.min(rmse_list)}, Algorithm:{names[np.argmin(np.array(rmse_list))]}')\n",
    "#saving rmse_per_algo\n",
    "pickle.dump(rmse_per_algo, open(f'./models/F1_rmse.pkl', 'wb'))\n",
    "print(rmse_per_algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('ml_dataset.csv')\n",
    "dataset_cleaned  = dataset[dataset.algo != 'Ensemble']\n",
    "dataset_cleaned = dataset_cleaned.drop(columns=['did'])\n",
    "dataset_cleaned['consumption']=dataset_cleaned['time']*dataset_cleaned['TDP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos=dataset_cleaned['algo'].unique()\n",
    "list_models=[{\"name\":\"Decision Tree\", \"clf\":decision_tree}, {\"name\":\"Random Forest\", \"clf\":random_forest},{\"name\":\"Linear Regression\",  \"clf\":linear_regression}, {\"name\":\"Nerual Network\", \"clf\":neural_network}]\n",
    "names=[x['name'] for x in list_models]\n",
    "rmse_per_algo=[]\n",
    "for algo in algos:\n",
    "    print(f'-------------DATASET {algo}--------------')\n",
    "    df=dataset_cleaned[dataset_cleaned['algo']==algo]\n",
    "    df=dataset_cleaned.drop(columns=['time','algo','country','TDP','F1'])\n",
    "    df=df.dropna()\n",
    "    #removing outliers in column consumption\n",
    "    k=0.2\n",
    "    Q1 = df['consumption'].quantile(0.25)\n",
    "    Q3 = df['consumption'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df['consumption'] < (Q1 - k * IQR)) |(df['consumption'] > (Q3 + k * IQR)))]\n",
    "    #log scaling for heavy-tailed distributions\n",
    "    for i in df.columns:\n",
    "        if i!='consumption':\n",
    "            df[i]=np.log(1+df[i])\n",
    "    X=df.loc[:,df.columns!='consumption'].to_numpy()\n",
    "    y=df['consumption'].to_numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    #selecting 7 features with highest mutual_information\n",
    "    selector = SelectKBest(mutual_info_regression, k=7).fit(X_train,y_train)\n",
    "    support = selector.get_support()\n",
    "    X_train = X_train[:,support]\n",
    "    X_test = X_test[:,support]\n",
    "    # run_autoML(X_train,X_test,y_train,y_test)\n",
    "    rmse_list=[]\n",
    "    clf_list=[]\n",
    "    for model in list_models:\n",
    "        clf,rmse=model['clf'](X_train,X_test, y_train,y_test)\n",
    "        rmse_list.append(rmse)\n",
    "        clf_list.append(clf)\n",
    "    data={'Model':names, 'RMSE':rmse_list}  \n",
    "    #plotting\n",
    "    df_plot=pd.DataFrame(data)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    ax = sns.barplot(x=\"Model\", y=\"RMSE\", data=df_plot)\n",
    "    filename='_'.join(algo.split())\n",
    "    plt.savefig(f'./plots/consumption_model_{filename}.png')\n",
    "    rmse_per_algo.append(np.min(np.array(rmse_list)))\n",
    "    #save best model\n",
    "    pickle.dump(clf_list[np.argmin(np.array(rmse_list))], open(f'./models/consumption_model_{filename}.sav', 'wb'))\n",
    "    #saving support features\n",
    "    support = selector.get_support()\n",
    "    pickle.dump(support, open(f'./models/support_consumption_{filename}.pkl', 'wb'))\n",
    "    print(f'Min rmse:{np.min(rmse_list)}, Algorithm:{names[np.argmin(np.array(rmse_list))]}')\n",
    "#saving rmse_per_algo\n",
    "pickle.dump(rmse_per_algo, open(f'./models/consumption_rmse.pkl', 'wb'))\n",
    "print(rmse_per_algo)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "769d560f7a9260275cfba8eac8dfb7a8ebd643a4b3237d9fce15021d62ac6fd5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}